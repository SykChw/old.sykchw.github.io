---
title: LSTM
draft: true
date: 2024-04-12
---

```mermaid
flowchart LR

mul1((* Wₕ))
mul2((* Wₓ))
add((+ b))
tanh([tanh])
yin[yₜ₋₁]
xin[Xₜ]
out[Yₜ]

yin --> mul1
xin --> mul2
tanh --> out

subgraph "RNN node"
mul1 & mul2 --> add --> tanh
end

subgraph "prev. hidden state"
yin
end

subgraph input
xin
end

subgraph output
out
end

```

**Recurrent Neural Networks (RNNs)** have a recurring set of weights that have been trained with the help of backpropagation and utilise inputs from both the previous hidden state and current input to predict a possible output prediction.

```mermaid
flowchart LR

mul1((* Wₕ))
mul2((* Wₓ))
add((+ b))
tanh([Activation
 tanh])
hin[hₜ₋₁]
hout[hₜ]
af[Softmax]
in[Xₜ]
out[Yₜ]

hin --> mul1
in --> mul2
tanh --> af & hout
af --> out

subgraph RNN
mul1 & mul2 --> add --> tanh
end

```
The function representing vanilla RNNs is:
$$
y_t = a \left(W_x*X_t + W_y * y_{t-1} + b \right)
$$
where $a(\dots)$ is the activation function used and $W_x, W_y$ are the weights corresponding to the current input and previous hidden state for the single node in the Recurrent Neural Network.

There are various improvements upon this architecture such as LSTMs whose individual node structure is shown below: 

```mermaid
flowchart LR

sig1[σ]
sig2[σ]
th1[tanh]
th2[tanh]
li([Cₜ₋₁])
lo([Cₜ])
si([hₜ₋₁])
so([hₜ])
mul1((×))
mul2((×))
sum1((sum))
sum2((sum))

li --> mul1 --> sum1 --> lo

si 


```
