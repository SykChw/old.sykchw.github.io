---
title: Neural Network Verification
draft: false
date: 2024-03-27
---
## Motivation

For any reasonably complex system, it is necessary to ensure that the system is acting as intended without any unpredictable behaviours that may end up compromising the system and/or the systems surrounding it. There are many safety critical systems whose malfunctioning can cause innumerable casualties and loss of life, thus for these systems we need to reliably prove that they have predictable outputs corresponding to determined inputs.

Formal Verification utilizes a lot of different mathematical tools to demonstrate the "correctness" of some system with respect to some desirable specification or property. The set of tools used for for Formal Verification is collectively known as Formal Methods and include such theories like logic, automata, type systems and formal languages. These can be used to check the behaviour of some system against a set of specifications in order to determine correctness of the system. Formal Verification can be imagined as a layer of abstraction on top of systems (both software and hardware) that provides some guarantees as to the functioning of these systems.

Below we will be building up to the application of Formal Methods to the field of Machine Learning in order to Verify Neural Networks.

## Introduction

### Background

One of the first demonstrations of the possibilities of verification came from Alan Turing's 1949 paper called ['Checking a large routine'](https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-8). In the paper the program for finding the factorial of a number was broken down into a simple sequential set of instructions that could either be either $True$ or $False$. The truth values for each of these assertions were then checked in order to prove that the program is a faithful implementation of the factorial mathematical function. This is known as proving the functional correctness of a program. Although this is the gold standard for demonstrating correctness, it is not possible to do so for Neural Networks as the tasks performed cannot be described mathematically. In a Neural Network we generally test for the following qualities:

- **Robustness**: This class of properties is violated when small perturbations to the inputs result in changes to the output. It also measures how well a model performs on new and unseen test data.
- **Safety**: This is a broad class of correctness properties that ensure that the program does not reach a 'bad' state. (e.g., a surgical robot should not be operating at 200mph)
- **Consistency**: This class of properties are violated only when the algorithms are not consistent with the real world. (e.g., tracking an object in free fall should be consistent with the laws of gravitation)

We will mostly be testing for Robustness, as that is the set of properties that are most widely tested for. It can also encapsulate the Safety and Consistency classes (as there is not well-defined boundary between them) if the specifications are designed accordingly.

<div style="background-color: #080D11;">
Alan Turning had also extensively thought about 'Networks that can Learn', which can be seen from his 1948 paper <a href=https://doi.org/10.1093/oso/9780198250791.003.0016>Intelligent Machinery</a> where he proposes Boolean NAND networks that can learn over time.
</div>

### Abstraction of Neural Network

We will not be dealing with the technicalities of the Neural Networks but rather used well-rounded mathematical abstractions that can be rigorously tested over. Since all Neural Networks can be though of as Directed Acyclic Graphs (DAGs), we will be treating them as such. More specifically they will be treated as dataflow graphs of operators over $\mathbb{R}$. The shape of these graphs will determine what specific architecture they belong to. Each node will perform some computation, whose dependencies are the edges. Thus, a neural network will be associated with some function:  
$$f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$$

For a Neural Network: $G = (V, E)$, we have: \
$V$: finite set of nodes \
$E \subseteq V \times V$: set of edges \
$V^{in} \subset V$ : input nodes \
$V^{o} \subset V$ : output nodes \
$n_v$ : total number of edges whose target is $v$ \
where, $n = |V^{in}|$ and $m = |V^{o}|$ for the network.

All neural networks are finite DAGs, there are classes of neural networks called Recurrent Neural Networks that have loops, but the number of iterations will depend on the size of the input. RNNs have self-loops that unroll based on the length of the input, which means that it reliably terminates. So while testing neural networks we will not test for loop termination and the explosion of possible program paths that comes with it.  

![[RNNs.drawio.png|500]]

For our purposes the following conditions must be met in a network:
- all nodes must be reachable from some input node
- every node can reach an output node
- fixed total ordering on edges $E$ and another one on nodes $V$.

Each node $v$ of the neural network is a function of the following form:
$$f_v: \mathbb{R}^{n_v} \rightarrow \mathbb{R}$$

Where each node takes in some vector, does some computation on it and return a single output number that is passed through an activation function (a non-linear function), for ease of analysis they are treated as belonging to two different nodes. Each of the elements of the vectors are the outputs of previous nodes. These relations can be recursively defined with the base case terminating at the input nodes. Therefore, for every non-input node $v \in V$, we have:  

![[node.png]]

Where each $(v_i, v)$ represents an edge connecting node $v_i$ to $v$. The ordering of these edges and nodes determines the inputs on the basis of which the computations will be done. Each node will have an $out(v)$ function which can be defined as follows:
$$out(v) = f_v(x_1, ..., x_{n_v})$$
where $x_i = out(v_i)$ for $i \in \{1, 2, ... , n_v\}$. This is the recursive definition wherein each input to the node $x_i$ can be defined as:
$$out(v) = f_v(out(v_1), out(v_2), ... , out(v_{n_v}))$$  
No sequence of operations are defined, only which nodes need what data to perform its computations. A more modified version of these graphs are known as **computation graphs**. $out(v_i)$ values can be computed in any topological ordering of graph nodes, as it needs to be ensured that all the inputs need to be computed before the target node itself.  


All vector computations need to be linear:
$$f(x) = \sum_{i=1}^{n} c_i x_i + b$$
or piece-wise linear:
$$  
f(x)\left\{\begin{array}{l}  
\sum_i c^{[1]}_i x_i + b^{[1]} \: , \: x \in S_1 \\  
\vdots \\  
\sum_i c^{[m]}_i x_i + b^{[m]} \: , \: x \in S_m  
\end{array}\right.  
$$
where $\cup_i S_i = \mathbb{R}^n$ and $\cap_i S_i = \varnothing$.   

<div style="background-color: #080D11;"> Note: Neural Networks are an instance of differential functions.</div>  

### Defining Specifications

We define a language that specifies some properties about the functioning of a neural net. This will enable us to later on make statements and verify them based on the specifying language.

**Specifications** are generally of the form:
$$
\begin{aligned}
\{precondition\} \\
r \leftarrow f(x) \\
\{postcondition\}
\end{aligned}
$$
where both preconditions and post-conditions are statements that specify some property that is adjacent to input and output respectively. Properties dictate the input-output behaviour of the network (and not the internals). Specifications help in quantifying some properties for accurate verification. Each specification can be thought of as being structured in the following way:  

$$\underbrace{for \ any \ inputs \ x, y, \dots \ that \ \dots}_{precondition} \ the \ neural \ network \ G \ produces \ \underbrace{output \ that \ \dots}_{pst condition}$$

Although every possible specification needed for complete verification cannot be made, multiple specifications can be combined together to test for stronger properties. Preconditions are generally predicates or Boolean functions defined over set of variables which act as inputs to the system, and  post condition is a Boolean predicate over the variables appearing in precondition ($x_i$) and assigned variables ($r_i$).  
For any values of $x_1, \dots , x_n$ that make the precondition true, let $r_1 = f(x_1)$, $r_2 = g(x_2) \dots$ , where $f(), g(), \dots$ are the computations on the input, then the post condition must also be *True*.  

$$
\begin{aligned}
\{precondition\} \\
r_1 \leftarrow f(x_1) \\
r_2 \leftarrow g(x_2) \\
\vdots \\
\{postcondition\}
\end{aligned}
$$  
If the post condition is **False**, then the correctness property is not true, i.e., the property does not hold.  

Consider: c is an actual greyscale image, each element of c is the intensity of a pixel $\in (0, 1)$; we can state the following specification about the brightness of c and its corresponding classification:  
$$
\begin{aligned}
\{|x-c| \le 0.1 \} \\
r_1 \leftarrow f(x) \\
r_2 \leftarrow f(c) \\
\{class(r_1) = class(r_2)\}
\end{aligned}
$$  
$r_1$ and  $r_2$ are vectors whose elements are probabilities for a belonging to a class with labels corresponding to the indexes. $class(r_1)$ and  $class(r_2)$ extract the indexes corresponding to the largest elements of the vectors $r_1$ and  $r_2$ respectively.  

<div style="background-color: #080D11;"> <b>Counterexamples:</b> Valuations of variables in precondition that falsifies the post condition.</div>

### Constraint-Based Satisfaction

A correctness property is taken and encoded as a set of constraints, solving which will help us to decide whether the property holds or not.

Let $fv(F)$ be the set of free variables appearing in the formula $F$. 

**Interpretation:** $I$ of $F$ is a map from variables present in $fv(F)$ to either $True$ or $False$. $I(F)$ denotes the formula where the variables have been replaced with their corresponding interpretations.  


***Example:***  
Let $F \triangleq (p \land q) \lnot r$, which means that $F$ is syntactically defined to be equal to the Boolean formula (as opposed to being semantically equivalent).  

$$
\begin{aligned}
\\ fv(F) = \{p, q, r\}
\\ I = \{ p \mapsto \text{True, } q \mapsto \text{False, } r \mapsto \text{True}
\} 
\\ I(F) \triangleq (\text{True} \land \text{False}) \lnot \text{True} 
\end{aligned}
$$

$eval(F)$: denotes the simplest form of $F$ we can get by evaluating repeatedly.  

$F$ is **satisfiable (SAT)** if there exists an $I$ s.t. $eval(I(F)) = True$, in which case, $I$ is the model of $F$: $I \models F$.   
$I \not \models F$ denotes $I$ isn't a model of $F$.  \
$I \not \models F$ if and only if $I \models \lnot F$.   \
$F$ is **unsatisfiable (UNSAT)** if $\forall I, \ eval(I(F)) = False$. 

**Validity:** If every interpretation $I$ is a model of $F$, then $F$ is valid.

Boolean satisfiability theories (SAT) can be generalised to more complex theories to include reals, vectors, strings, arrays, etc.; the problem of determining whether statements within these theories are true is known as **Satisfiability Modulo Theories** or simply **SMT**. We will be extensively using a first-order logic system called **Linear Real Arithmetic (LRA)** as it can represent a large class of neural networks and is decidable.  
In LRA, each propositional variable is replaced by a linear inequality of the form:
$$
\sum_{i=1}^{n} c_i x_i + b \le 0 \ \ \text{ OR } \ \ \sum_{i=1}^{n} c_i x_i + b \lt 0
$$  
where $c_i, b \in \mathbb{R}$.  

$$
(\ \underbrace{x+y \le 0}_{p} \ \land \ \underbrace{x-2y \lt 10}_{q} \ ) \ \lor \underbrace{x \ge 100}_{r}
$$  
An interpretation $I$ of $F$ is an assignment of every free variable to a real number.

### Encoding Neural Networks

We need to translate Neural Networks into a formula in LRA such that we can use SMT Solvers (specialised software that tests satisfiability for SMT problems).  

$f_v$: Function in node. \
$I$: Model. \
$F_v$: Encoding for node. \
$R_g$: Relational mapping for neural network.

<div style="background-color: #080D11;">Generally Variables with a subscript G refers to the overall neural network as a graph and those with a subscript v refers to individual nodes or a collection of nodes.</div>

Whole Network can be defined as a binary relation:  
$$
R_G = \{ (a, b) | \ a \in \mathbb{R}^{n},\ b = f_G(a) \}
$$  
$R_v$, $f_v$ define the same for a single node $v$ in $G$ network. 

**For a single neuron with corresponding function $f_v: \mathbb{R} → \mathbb{R}$, both can be defined as:**

$$
f_v(x) = x + 1 \ \ \text{and} \ \ R_v = \{ (a, a+1)| \ a \in \mathbb{R} \}
$$
$$
F_v \triangleq v^o = v^{in, 1} + 1
$$  
Models of $F_v$ will be of the form $\{ v^{in,1} \mapsto a, v^o \mapsto a +1 \}$ and have one-to-one correspondence with the elements of $R_v$.

**For two inputs:**

$$F_v \triangleq v^o = v^{in, 1} + 1.5 v^{in, 2}$$  
**Encoding for a single node**

Formalizing the operation $f_v$ of some node $v$. We assume that the function $f_v : \mathbb{R}^{n_v} \rightarrow \mathbb{R}$ is piecewise linear, i.e., of the form
$$
\begin{equation}
f(x)=\left\{\begin{array}{l}
\sum_j c^1_j x_j + b \ \ \ \ \ \text{ if } \ S_1\\
\vdots \\
\sum_j c^l_j x_j + b \ \ \ \ \ \text{ if } \ S_l
\end{array}\right.
\end{equation}
$$  
where j ranges from $1$ to $n_v$ 
$$
F_v \triangleq \bigwedge_{i=1}^l\left[S_i \Rightarrow\left(v^{\mathrm{o}}=\sum_{j=1}^{n_v} c_j^i \cdot v^{\mathrm{in}, j}+b^i\right)\right]
$$